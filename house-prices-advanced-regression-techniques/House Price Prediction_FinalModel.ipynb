{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "House Prices: Advanced Regression Techniques\n",
    "Input Missing Values\n",
    "Data Distribution\n",
    "Data Transformation from numerical to categorical column values\n",
    "Label Encoding some categorical varibles that may contain some information in their ordering set.\n",
    "Getting Dummy Variables for categorical features\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.stats import skew\n",
    "from sklearn.linear_model import Ridge,RidgeCV,ElasticNet,LassoCV,LassoLarsCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read_csv(\"C:/Users/320061150/HealthCare_Components_Task/Kaggle/house-prices-advanced-regression-techniques/train.csv\")\n",
    "testData = pd.read_csv(\"C:/Users/320061150/HealthCare_Components_Task/Kaggle/house-prices-advanced-regression-techniques/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Concatenating from MSSubClass - SaleCondition. Excluding ID and SalePrice column\n",
    "\"\"\"\n",
    "allData = pd.concat((trainData.loc[:,'MSSubClass':'SaleCondition'],testData.loc[:,'MSSubClass':'SaleCondition']))\n",
    "allData\n",
    "   \n",
    "\"\"\"\n",
    "Data preprocessing:\n",
    "We're not going to do anything fancy here:\n",
    "\n",
    "First I'll transform the skewed numeric features by taking log(feature + 1) - this will make the features more normal\n",
    "Create Dummy variables for the categorical features\n",
    "Replace the numeric missing values (NaN's) with the mean of their respective columns\n",
    "\"\"\"\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "prices = pd.DataFrame({\"price\":trainData[\"SalePrice\"], \"log(price + 1)\":np.log1p(trainData[\"SalePrice\"])})\n",
    "prices.hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#log transform the target:\n",
    "trainData[\"SalePrice\"] = np.log1p(trainData[\"SalePrice\"])\n",
    "\n",
    "\"\"\"\n",
    "log transform skewed numeric features and transforming data int dummy variables.\n",
    "\n",
    "\"\"\"\n",
    "numeric_feats = allData.dtypes[allData.dtypes != 'object'].index\n",
    "skewed_feats = trainData[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "skewed_feats = skewed_feats[skewed_feats> 0.75]\n",
    "skewed_feats = skewed_feats.index\n",
    "\n",
    "allData[skewed_feats] = np.log1p(allData[skewed_feats])\n",
    "allData = pd.get_dummies(allData)\n",
    "allData = allData.fillna(allData.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = allData[:trainData.shape[0]]\n",
    "X_test = allData[trainData.shape[0]:]\n",
    "y = trainData.SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Models\n",
    "Now we are going to use regularized linear regression models from the scikit learn module. I'm going to try both l_1(Lasso) and l_2(Ridge) regularization. I'll also define a function that returns the cross-validation rmse error so we can evaluate our models and pick the best tuning par\n",
    "\"\"\"\n",
    "def rmse_cv(model):\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lasso Regression\n",
    "The lasso performs even more bettwe\n",
    "\"\"\"\n",
    "model_lasso =  LassoCV(alphas=[1,0.1,0.001,0.0005]).fit(X_train,y)\n",
    "rmse_cv(model_lasso).mean()\n",
    "\"\"\"\n",
    "Nice! The lasso performs even better so we'll just use this one to predict on the test set. Another neat thing about the Lasso is that it does feature selection for you - setting coefficients of features it deems unimportant to zero. Let's take a look at the coefficients:\n",
    "\"\"\"\n",
    "coef = pd.Series(model_lasso.coef_,index=X_train.columns)\n",
    "print(\"Lasso picked \" +str(sum(coef !=0)) + \" variables and elimated the other \" + str(sum(coef == 0))+\" variables\")\n",
    "\n",
    "\"\"\"\n",
    "Identifying the important coefficients\n",
    "\"\"\"\n",
    "\n",
    "imp_coef = pd.concat([coef.sort_values().head(10),coef.sort_values().tail(10)])\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Coefficients in the Lasso Model\")\n",
    "\n",
    "lasso_preds = pd.DataFrame({'SalePrice':np.expm1(model_lasso.predict(X_test))})\n",
    "lasso_preds['Id'] = testData['Id']\n",
    "lasso_preds[['Id','SalePrice']]\n",
    "lasso_preds.columns = ['SalePrice','Id']\n",
    "lasso_preds.to_csv('sample_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
